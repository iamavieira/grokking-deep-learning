{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Function: A function we applied to \n",
    "the neurons in a layer during prediction.\n",
    "A function that takes a number and returns another.\n",
    "There's an infinite number ofÂ functions out there, but a good activation function follows some contraints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_First constraint:\n",
    "it must be continuous and infinite in domain_**.\n",
    "A function **f** is continuous when for every value _c_ in its domain, $f(c)$ is defined, and as x approaches c, $f(x) = f(c)$ \n",
    "$$\\lim_{x\\to\\c} f(x) = f(c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, simply put, its graph is an unbroken curve, with no jumps, \n",
    "holes or vertical asymptotes in its domain. There's no input x for which we can't compute an output y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def continuous_function(x):\n",
    "    return x * x;\n",
    "\n",
    "x = range(-100, 101, 10);\n",
    "y = zip([continuous_function(value) for value in x]);\n",
    "\n",
    "plt.plot(list(x), list(y));\n",
    "plt.ylabel('y (output)');\n",
    "plt.xlabel('x (input)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Constraint: Good activation functions are monotonic. \n",
    "Never changing direction i.e. always increasing or always decreasing**.\n",
    "\n",
    "This constraint is not technically a requirement. Unlike functions that have missing values(non-continuous), we can optimize functions that aren't monotonic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def monotonic_function(value):\n",
    "    return value;\n",
    "\n",
    "y = zip([monotonic_function(value) for value in x]);\n",
    "\n",
    "plt.plot(list(x), list(y));\n",
    "\n",
    "#The previous function i.e. y = x*x, isn't monotonic for\n",
    "#two different values of x can generate the same y. It changes direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for non-monotonic functions, consider the implication of having \n",
    "multiple 'perfect' configuration of weights to get to the same\n",
    "specific output. We are likely to find the answer if it can be found in multiple places. In the other hand, we no longer have a correct direction to move our weights to reduce the error for, theoretically, either direction would do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third Constraint: \n",
    "Good activation functions are nonlinear(i.e. they squiggle and turn).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The secret sauce in sometimes correlation:\n",
    "Selectively correlate to input neurons such that a quite negative\n",
    "value from one input could reduce how much a neuron correlated with any input at all(by simply forcing the neuron to drop to zero in the case of 'relu'). Curved functions facilitate this.\n",
    "\n",
    "When we have 'straight line' functions(i.e. linear functions) we simply scale the weighted average coming in. Simply scaling something doesn't affect how correlated a neuron is to its inputs. It just makes the collective correlation that is represented louder or softer.\n",
    "\n",
    "**Given a neuron with an activation function, we want 0ne incoming signal to be able to increase or decrease how correlated a neuron is to all of the other incoming signals**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there's no need to account for derivatives at linear functions,\n",
    "#for it is simply scaling the weighted average coming in. 1:1;\n",
    "linear = lambda x: 2 * x + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Relu activation function:**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(input):\n",
    "    return (input > 0) * input;\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return (output > 0);\n",
    "\n",
    "input = [ value for value in range(-10, 10) ];\n",
    "output = [ relu(value) for value in input ]\n",
    "deriv = [ relu2deriv(value) for value in output ]\n",
    "print(output);\n",
    "plt.plot(input, output);\n",
    "plt.plot(input, deriv);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forth Constraint: \n",
    "Good activaton functions( and their derivatives ) \n",
    "should be efficiently computable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is easy. One is going to call this function a LOT\n",
    "( sometimes billions of times), so one does not want it to be slow.\n",
    "\n",
    "In fact, most recent activation functions became popular due to their effortless computation at the expense of its expressiveness( 'relu' is a good example of it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard hidden layer activation functions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Sigmoid: The bread and butter_**\n",
    "\n",
    "$$\\sigma(sigma) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate values between zero and one. Can, and most of the time is used, in the output layer, as a way to get probabilities out of each neuron. 'Varying degrees of positive correlation'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigmoid = lambda input: 1/(1+np.exp(-input));\n",
    "sigmoid2deriv = lambda output: output * (1 - output);\n",
    "\n",
    "input = range(-10, 10);\n",
    "output = [ sigmoid(value) for value in input ];\n",
    "deriv = [ sigmoid2deriv(value) for value in output ] \n",
    "\n",
    "plt.plot(input, output);\n",
    "plt.plot(output, deriv);\n",
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tanh(hyperbolic tangent)**: generate values between -1 and 1; It's great for hidden layers as it can throw some negative correlation and help us modeling our selective correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$sinh x = \\frac{e^x - e^{-x}}{2}$\n",
    "\n",
    "$$cosh x = \\frac{e^x + e^{-x}}{2}$$\n",
    "\n",
    "$$tanh x = \\frac{sinhx}{coshx} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(input):\n",
    "    return sinh(input) / cosh(input)\n",
    "def sinh(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / 2\n",
    "def cosh(x):\n",
    "    return (np.exp(x) + np.exp(-x)) / 2\n",
    "\n",
    "def tanh2deriv(output):\n",
    "    return 1 - output ** 2\n",
    "    \n",
    "input = range(-100, 101, 10);\n",
    "output = [ tanh(value) for value in input ]\n",
    "deriv = [ tanh2deriv(value) for value in output ]\n",
    "\n",
    "plt.plot(input, output);\n",
    "#plt.plot(input, np.tanh(input)); built-in tanh;\n",
    "plt.plot(output, deriv);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  9.3 Standard output layer activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chosen based on output layer.\n",
    "\n",
    "Three major types of output layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data values(No activation function): output values are outside of tanh or sigmoid(i.e. -1 to 1), so there's no need to use them. \n",
    "Trask states, in such cases, that he would personally opt to train the network without an activation function on the output at all. e.g. Predicting temperature based on surroundings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unrelated Yes/No probabilities(sigmoid):\n",
    "Multiple binary probabilities in the same network.\n",
    "An example of it is our 'Multiple inputs and outputs' network, back at chapter 5;\n",
    "We had whether the team would win, whether there would be injuries and the morale of the team(happy or sad);\n",
    "Even though those predictions are separate, for hidden layers, perfectly predicting one could mean increased chances of getting the other right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one probabilities(Softmax):\n",
    "The most common case - predicting a single label out of many.\n",
    "softmax - The more likely it's one of the labels, the less likely it's any of the others.\n",
    "Where softmax asks 'which digit seems the best fit for this input?'\n",
    "sigmoid says 'you better believe it's only digit nine and doesn't have anything in common with other MNIST digits'.\n",
    "When we use sigmoid instead of softmax in our MNIST network we are bound to, even if we predicted perfectly(0 for everything except our label), we still end up backpropagating error, because for sigmoid it's not only about predicting the highest value for the true label, but predicting 0 everywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.g.\n",
    "    should predict nine:\n",
    "    raw_dot_product_values = (0, 0, 0, 0, 0, 0, 0, 0, 0, 1);\n",
    "    sigmoid(raw_dot_product_values) => (.5, .5 ... .5, .99)\n",
    "    #We'll backpropagate this thing, configuring an huge weight update, even though we already predicted perfectly.\n",
    "    sigmoid_msquared_error => (.25, .25, .25 ... .25, 0); \n",
    "    softmax(raw_dot_product_values) => raw_dot_product_values;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax : raises each input value exponentially and then divides by the layers sum. \n",
    "\n",
    "The higher the network predicts one value, the lower it predicts all others. That's the cool thing about it, given that it increases what is called 'sharpness of attenuation' - It encourages the output to predict with a very high probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(input):\n",
    "    exponential_raise = np.exp(input)\n",
    "    return exponential_raise / np.sum(exponential_raise);\n",
    "\n",
    "def softmax2deriv(output, binary_goal_pred):\n",
    "    tmp = output - binary_goal_pred;\n",
    "    return tmp / len(binary_goal_pred)\n",
    "\n",
    "input = range(-10, 10);\n",
    "output = softmax(list(input));\n",
    "true = np.zeros(output.shape)\n",
    "true[16] = 1;\n",
    "deriv = softmax2deriv(output, true);\n",
    "\n",
    "plt.plot(input, output);\n",
    "plt.plot(output, deriv);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun1(x):\n",
    "    return 10*x\n",
    "\n",
    "def fun2(x):\n",
    "    return 5*x+20\n",
    "\n",
    "MAX_X = 100# A maximum value for x goes here \n",
    "x = range(MAX_X);\n",
    "\n",
    "y1,y2 = zip(*[ (fun1(val), fun2(val)) for val in range(MAX_X) ])\n",
    "plt.plot(x, y1)\n",
    "plt.plot(x, y2)\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
