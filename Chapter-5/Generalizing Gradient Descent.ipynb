{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, weight = [[8.5, .65, 1.2], [.2, 5, 8]];\n",
    "alpha, goal_pred, iteration = (.001, 1, 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(vec_a, vec_b):\n",
    "    assert(len(vec_a) == len(vec_b));\n",
    "    output = 0;\n",
    "    for i in range(len(vec_a)):\n",
    "        output += vec_a[i] * vec_b[i];\n",
    "    return output;\n",
    "\n",
    "def calc_scalar_vec(scalar, vec):\n",
    "    output = [];\n",
    "    for i in range(len(vec)):\n",
    "        output.append(scalar * vec[i]);\n",
    "    return output;\n",
    "\n",
    "def calc_weight(weight, weight_delta):\n",
    "    output = [];\n",
    "    for i in range(len(weight)):\n",
    "        output.append(weight[i] - weight_delta[i] * alpha);\n",
    "    return output;\n",
    "\n",
    "def neural_network(input, weight):\n",
    "    for i in range(iteration):\n",
    "        pred = dot(input, weight);\n",
    "        msquared_error = (pred - goal_pred) ** 2;\n",
    "        delta = pred - goal_pred;\n",
    "        weight_delta = calc_scalar_vec(delta, input);\n",
    "        weight = calc_weight(weight, weight_delta);\n",
    "        print('Pred: ' + str(pred) + ' Error: ' + str(msquared_error));\n",
    "        if(msquared_error == 0):\n",
    "            break;\n",
    "            \n",
    "neural_network(weight, input);         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, weight = np.asarray(input), np.asarray(weight)\n",
    "alpha = .1\n",
    "def neural_network(input, weight):\n",
    "    for i in range(iteration):\n",
    "        pred = input.dot(weight)\n",
    "        msquared_error = (pred - goal_pred) ** 2\n",
    "        delta = pred - goal_pred;\n",
    "        weight_delta = delta * input;\n",
    "        weight -= weight_delta * alpha\n",
    "        print('Pred: ' + str(pred) + ' Error: ' + str(msquared_error) \\\n",
    "              + ' Weight: ' + str(weight) + ' Weight_delta: ' + str(weight_delta))\n",
    "              #+ ' Delta: ' + str(delta));\n",
    "        if( msquared_error == 0):\n",
    "            break;\n",
    "        \n",
    "neural_network(input, weight);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha was set lower given the high value of input a, \n",
    "that was causing the network to overshoot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we freezed one weight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, weight = [[8.5, .65, 1.2], [.2, 5, 8]];\n",
    "alpha, goal_pred, iteration = (.1, 1, 400);\n",
    "\n",
    "input, weight = np.asarray(input), np.asarray(weight)\n",
    "\n",
    "def neural_network(input, weight):\n",
    "    for i in range(iteration):\n",
    "        pred = input.dot(weight)\n",
    "        msquared_error = (pred - goal_pred) ** 2\n",
    "        delta = pred - goal_pred\n",
    "        weight_delta = delta * input\n",
    "        weight -= weight_delta * alpha\n",
    "        weight[0] = 0\n",
    "        print('Pred: ' + str(pred) + ' Error: ' + str(msquared_error) \\\n",
    "              + ' Weight: ' + str(weight) + ' Weight_delta: ' + str(weight_delta))\n",
    "              #+ ' Delta: ' + str(delta));\n",
    "        if( msquared_error == 0):\n",
    "            break;\n",
    "        \n",
    "neural_network(input, weight);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent with multiple outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, alpha = 8.5, .01;\n",
    "weight, goal_pred = [[.2, 5, 8], [20, 500, 2]];\n",
    "def msquared(pred, goal_pred): \n",
    "    assert(len(pred) == len(goal_pred));\n",
    "    error = [];\n",
    "    for i in range(len(goal_pred)):\n",
    "        error.append((pred[i] - goal_pred[i]) ** 2);\n",
    "    return error;\n",
    "\n",
    "def calc_delta(pred, goal_pred):\n",
    "    output = [];\n",
    "    for i in range(len(goal_pred)):\n",
    "        output.append(pred[i] - goal_pred[i]);\n",
    "    return output;\n",
    "\n",
    "def neural_network(input, weight):\n",
    "    for i in range(iteration):\n",
    "        print('Iteration: ' + str(i))\n",
    "        pred = calc_scalar_vec(input, weight);\n",
    "        msquared_error = msquared(pred, goal_pred);\n",
    "        delta = calc_delta(pred, goal_pred);\n",
    "        weight_delta = calc_scalar_vec(input, delta);\n",
    "        weight = calc_weight(weight, weight_delta);\n",
    "        print('Pred: ' + str(pred) + ' \\nError: ' + str(msquared_error));\n",
    "        if (not np.any(msquared_error)): break;\n",
    "        \n",
    "neural_network(input, weight);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, weight, goal_pred = np.asarray(8.5), np.array([.2, 5, 8]), np.array([20, 500, 2])\n",
    "alpha = .01\n",
    "\n",
    "def neural_network(input, weight):\n",
    "    for i in range(iteration):\n",
    "        print('Iteration: ' + str(i))\n",
    "        pred = input.dot(weight);\n",
    "        msquared_error = (pred - goal_pred) ** 2\n",
    "        delta = pred - goal_pred;\n",
    "        weight_delta = input * delta;\n",
    "        weight -= weight_delta * alpha;\n",
    "        print('Pred: ' + str(pred) + ' Error: ' + str(msquared_error));\n",
    "        print('Delta: ' + str(delta) + ' Weight_delta: ' + str(weight_delta));\n",
    "        if(not np.any(msquared_error)): break;\n",
    "            \n",
    "neural_network(input, weight);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent with multiple inputs, multiples outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .0001;\n",
    "input, goal_pred = [ [85, 32, 4], [.2, .3, .6] ];\n",
    "weights = [ [0, 0, 0],\n",
    "           [0, 0, 0],\n",
    "           [0, 0, 0] ];\n",
    "\n",
    "def vec_matrix_mult(vec, matrix):\n",
    "    output = [];\n",
    "    for i in range(len(matrix)):\n",
    "        assert(len(vec) == len(matrix[i]));\n",
    "        output.append(dot(vec, matrix[i]));\n",
    "    return output;\n",
    "\n",
    "def calc_weight_delta(input, delta):\n",
    "    assert(len(input) == len(delta));\n",
    "    output = []; \n",
    "    for i in range(len(delta)): \n",
    "        output.append(calc_scalar_vec(delta[i], input));\n",
    "    return output;\n",
    "\n",
    "def wrap(weights, weight_delta):\n",
    "    assert(len(weights) == len(weight_delta))\n",
    "    output = []\n",
    "    for i in range(len(weight)):\n",
    "        output.append(calc_weight(weights[i], weight_delta[i]));\n",
    "    return output;\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    for i in range(iteration):\n",
    "        pred = vec_matrix_mult(input, weights);\n",
    "        msquared_error = msquared(pred, goal_pred);\n",
    "        delta = calc_delta(pred, goal_pred);\n",
    "        weight_delta = calc_weight_delta(input, delta);\n",
    "        weights = wrap(weights, weight_delta);\n",
    "        print('Pred: ' + str(pred) + ' Error: ' + str(msquared_error));\n",
    "        if(not np.any(msquared_error)): break;\n",
    "        print('---------------');\n",
    "        \n",
    "neural_network(input, weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: [13.9766325  11.60911248  8.97160754] Error: [3.57201360e+01 1.66783117e+05 9.06180931e+04]\n",
      "Pred: [ 13.7017074  105.53901661  85.13179084] Error: [3.25094673e+01 9.88857101e+04 5.05657115e+04]\n",
      "Pred: [ 13.43942886 177.86504279 142.02344775] Error: [2.95873864e+01 5.86293375e+04 2.82161221e+04]\n",
      "Pred: [ 13.18921514 233.55608295 184.52151547] Error: [2.69279537e+01 3.47613342e+04 1.57448501e+04]\n",
      "Pred: [ 12.95051124 276.43818387 216.26757206] Error: [   24.50756154 20609.99505071  8785.76804792]\n",
      "Pred: [ 12.72278772 309.45740158 239.98187633] Error: [   22.30472388 12219.66606557  4902.53764265]\n",
      "Pred: [ 12.50553949 334.88219922 257.69646162] Error: [  20.29988608 7245.04001027 2735.66012744]\n",
      "Pred: [ 12.29828467 354.4592934  270.92925683] Error: [  18.47525112 4295.58422209 1526.52297205]\n",
      "Pred: [ 12.10056358 369.53365592 280.81415485] Error: [  16.81462165 2546.85188528  851.81355711]\n",
      "Pred: [ 11.91193765 381.14091505 288.19817367] Error: [  15.30325619 1510.02848278  475.31963119]\n",
      "Pred: [ 11.73198852 390.07850459 293.71403573] Error: [ 13.92773831 895.29588744 265.23263208]\n",
      "Pred: [ 11.56031705 396.96044854 297.83438469] Error: [ 12.67585748 530.82093166 148.00219579]\n",
      "Pred: [ 11.39654246 402.25954537 300.91228537] Error: [ 11.53650071 314.72373038  82.58655727]\n",
      "Pred: [ 11.24030151 406.33984994 303.21147717] Error: [ 10.49955388 186.59969974  46.08404224]\n",
      "Pred: [ 11.09124764 409.48168445 304.92897344] Error: [  9.55581198 110.63496198  25.71531032]\n",
      "Pred: [ 10.94905025 411.90089703 306.21194316] Error: [ 8.69689737 65.59546896 14.3493746 ]\n",
      "Pred: [ 10.81339394 413.76369071 307.17032154] Error: [ 7.91518545 38.89155354  8.00708017]\n",
      "Pred: [ 10.68397782 415.19804185 307.88623019] Error: [ 7.20373692 23.0588021   4.4680228 ]\n",
      "Pred: [ 10.56051484 416.30249222 308.42101395] Error: [ 6.55623623 13.67156376  2.49319693]\n",
      "Pred: [ 10.44273115 417.15291901 308.82049742] Error: [5.96693549 8.10587016 1.39122633]\n",
      "Pred: [ 10.33036552 417.80774764 309.11891158] Error: [5.43060346 4.80597041 0.77631681]\n",
      "Pred: [ 10.22316871 418.31196568 309.34182695] Error: [4.9424791  2.84945986 0.43319177]\n",
      "Pred: [ 10.12090295 418.70021358 309.50834473] Error: [4.49822931 1.68944475 0.24172491]\n",
      "Pred: [ 10.02334141 418.99916445 309.63273351] Error: [4.09391047 1.00167179 0.13488467]\n",
      "Pred: [  9.93026771 419.22935663 309.72565193] Error: [3.72593342 0.59389121 0.07526686]\n",
      "Pred: [  9.84147539 419.4066046  309.79506199] Error: [3.39103162 0.3521181  0.04199959]\n",
      "Pred: [  9.75676752 419.54308555 309.84691131] Error: [3.08623213 0.20877082 0.02343615]\n",
      "Pred: [  9.67595622 419.64817587 309.88564275] Error: [2.80882924 0.12378022 0.01307758]\n",
      "Pred: [  9.59886223 419.72909542 309.91457513] Error: [2.55636044 0.07338929 0.00729741]\n",
      "Pred: [  9.52531457 419.79140347 309.93618762] Error: [2.32658454 0.04351251 0.00407202]\n",
      "Pred: [  9.4551501  419.83938067 309.95233216] Error: [2.11746181 0.02579857 0.00227222]\n",
      "Pred: [  9.38821319 419.87632312 309.96439212] Error: [1.92713587e+00 1.52959708e-02 1.26792111e-03]\n",
      "Pred: [  9.32435539 419.9047688  309.97340091] Error: [1.75391719e+00 9.06898111e-03 7.07511389e-04]\n",
      "Pred: [  9.26343504 419.92667198 309.98013048] Error: [1.59626810e+00 5.37699890e-03 3.94797722e-04]\n",
      "Pred: [  9.20531703 419.94353742 309.98515747] Error: [1.45278914e+00 3.18802265e-03 2.20300682e-04]\n",
      "Pred: [  9.14987244 419.95652382 309.98891263] Error: [1.32220664e+00 1.89017863e-03 1.22929763e-04]\n",
      "Pred: [  9.09697831 419.96652334 309.99171773] Error: [1.20336142e+00 1.12068691e-03 6.85959144e-05]\n",
      "Pred: [  9.04651731 419.97422297 309.99381315] Error: [1.09519848e+00 6.64455268e-04 3.82771376e-05]\n",
      "Pred: [  8.99837751 419.98015169 309.99537842] Error: [9.96757660e-01 3.93955528e-04 2.13589873e-05]\n",
      "Pred: [  8.95245215 419.9847168  309.99654768] Error: [9.07165094e-01 2.33576233e-04 1.19185071e-05]\n",
      "Pred: [  8.90863935 419.98823194 309.99742112] Error: [8.25625467e-01 1.38487348e-04 6.65063424e-06]\n",
      "Pred: [  8.86684194 419.99093859 309.99807357] Error: [7.51414947e-01 8.21091489e-05 3.71111376e-06]\n",
      "Pred: [  8.82696721 419.99302271 309.99856096] Error: [6.83874766e-01 4.86825144e-05 2.07083488e-06]\n",
      "Pred: [  8.78892672 419.99462749 309.99892504] Error: [6.22405367e-01 2.88638628e-05 1.15554450e-06]\n",
      "Pred: [  8.75263609 419.99586317 309.999197  ] Error: [5.66461083e-01 1.71133842e-05 6.44804231e-07]\n",
      "Pred: [  8.71801483 419.99681464 309.99940016] Error: [5.15545295e-01 1.01465255e-05 3.59806564e-07]\n",
      "Pred: [  8.68498615 419.99754727 309.99955192] Error: [4.69206022e-01 6.01587498e-06 2.00775301e-07]\n",
      "Pred: [  8.65347678 419.9981114  309.99966528] Error: [4.27031908e-01 3.56681227e-06 1.12034425e-07]\n",
      "Pred: [  8.62341685 419.99854578 309.99974997] Error: [3.88648572e-01 2.11476300e-06 6.25162175e-08]\n",
      "Pred: [  8.59473968 419.99888025 309.99981323] Error: [3.53715283e-01 1.25384298e-06 3.48846120e-08]\n",
      "Pred: [  8.56738165 419.99913779 309.99986048] Error: [3.21921939e-01 7.43403503e-07 1.94659274e-08]\n",
      "Pred: [  8.5412821  419.9993361  309.99989578] Error: [2.92986307e-01 4.40763937e-07 1.08621627e-08]\n",
      "Pred: [  8.51638312 419.9994888  309.99992215] Error: [2.66651526e-01 2.61328938e-07 6.06118455e-09]\n",
      "Pred: [  8.4926295  419.99960637 309.99994184] Error: [2.42683820e-01 1.54941928e-07 3.38219553e-09]\n",
      "Pred: [  8.46996854 419.99969691 309.99995656] Error: [2.20870428e-01 9.18650688e-08 1.88729555e-09]\n",
      "Pred: [  8.44834999 419.99976662 309.99996755] Error: [2.01017710e-01 5.44667993e-08 1.05312790e-09]\n",
      "Pred: [  8.42772589 419.9998203  309.99997576] Error: [1.82949434e-01 3.22933653e-08 5.87654848e-10]\n",
      "Pred: [  8.4080505  419.99986163 309.99998189] Error: [1.66505207e-01 1.91467363e-08 3.27916694e-10]\n",
      "Pred: [  8.38928017 419.99989345 309.99998647] Error: [1.51539053e-01 1.13521000e-08 1.82980466e-10]\n",
      "Pred: [  8.37137329 419.99991796 309.9999899 ] Error: [1.37918117e-01 6.73066006e-09 1.02104747e-10]\n",
      "Pred: [  8.35429011 419.99993683 309.99999245] Error: [1.25521485e-01 3.99060834e-09 5.69753681e-11]\n",
      "Pred: [  8.33799277 419.99995136 309.99999436] Error: [1.14239112e-01 2.36603169e-09 3.17927681e-11]\n",
      "Pred: [  8.3224451  419.99996255 309.99999579] Error: [1.03970844e-01 1.40282019e-09 1.77406502e-11]\n",
      "Pred: [  8.30761263 419.99997116 309.99999685] Error: [9.46255282e-02 8.31732093e-10 9.89944265e-12]\n",
      "Pred: [  8.29346245 419.99997779 309.99999765] Error: [8.61202073e-02 4.93133958e-10 5.52397826e-12]\n",
      "Pred: [  8.27996317 419.9999829  309.99999824] Error: [7.83793785e-02 2.92379123e-10 3.08242939e-12]\n",
      "Pred: [  8.26708487 419.99998683 309.99999869] Error: [7.13343265e-02 1.73351581e-10 1.72002345e-12]\n",
      "Pred: [  8.25479896 419.99998986 309.99999902] Error: [6.49225119e-02 1.02780153e-10 9.59788515e-13]\n",
      "Pred: [  8.24307821 419.99999219 309.99999927] Error: [5.90870168e-02 6.09383526e-11 5.35570613e-13]\n",
      "Pred: [  8.23189661 419.99999399 309.99999945] Error: [5.37760394e-02 3.61303488e-11 2.98853226e-13]\n",
      "Pred: [  8.22122937 419.99999537 309.99999959] Error: [4.89424339e-02 2.14216841e-11 1.66762806e-13]\n",
      "Pred: [  8.21105282 419.99999644 309.99999969] Error: [4.45432922e-02 1.27009169e-11 9.30551435e-14]\n",
      "Pred: [  8.20134439 419.99999726 309.99999977] Error: [4.05395629e-02 7.53037316e-12 5.19256065e-14]\n",
      "Pred: [  8.19208255 419.99999789 309.99999983] Error: [3.68957048e-02 4.46475816e-12 2.89749663e-14]\n",
      "Pred: [  8.18324675 419.99999837 309.99999987] Error: [3.35793713e-02 2.64715514e-12 1.61683013e-14]\n",
      "Pred: [  8.1748174  419.99999875 309.99999991] Error: [3.05611231e-02 1.56949834e-12 9.02204449e-15]\n",
      "Pred: [  8.1667758  419.99999904 309.99999993] Error: [2.78141671e-02 9.30555625e-13 5.03438970e-15]\n",
      "Pred: [  8.15910411 419.99999926 309.99999995] Error: [2.53141185e-02 5.51726439e-13 2.80922981e-15]\n",
      "Pred: [  8.15178532 419.99999943 309.99999996] Error: [2.30387843e-02 3.27118622e-13 1.56757674e-15]\n",
      "Pred: [  8.1448032  419.99999956 309.99999997] Error: [2.09679662e-02 1.93948619e-13 8.74724664e-16]\n",
      "Pred: [  8.13814225 419.99999966 309.99999998] Error: [1.90832815e-02 1.14992153e-13 4.88102743e-16]\n",
      "Pred: [  8.13178771 419.99999974 309.99999998] Error: [1.73679998e-02 6.81788270e-14 2.72366183e-16]\n",
      "Pred: [  8.12572547 419.9999998  309.99999999] Error: [1.58068945e-02 4.04232281e-14 1.51983131e-16]\n",
      "Pred: [  8.1199421  419.99999985 309.99999999] Error: [1.43861076e-02 2.39669400e-14 8.48083598e-17]\n",
      "Pred: [  8.11442476 419.99999988 309.99999999] Error: [1.30930267e-02 1.42100009e-14 4.73240447e-17]\n",
      "Pred: [  8.10916123 419.99999991 309.99999999] Error: [1.19161731e-02 8.42511445e-15 2.64074257e-17]\n",
      "Pred: [  8.10413981 419.99999993 310.        ] Error: [1.08450998e-02 4.99525494e-15 1.47355633e-17]\n",
      "Pred: [  8.09934938 419.99999995 310.        ] Error: [9.87029886e-03 2.96169253e-15 8.22236278e-18]\n",
      "Pred: [  8.09477931 419.99999996 310.        ] Error: [8.98311692e-03 1.75598350e-15 4.58805137e-18]\n",
      "Pred: [  8.09041946 419.99999997 310.        ] Error: [8.17567844e-03 1.04111869e-15 2.56027328e-18]\n",
      "Pred: [  8.08626016 419.99999998 310.        ] Error: [7.44081576e-03 6.17280063e-16 1.42861436e-18]\n",
      "Pred: [  8.0822922  419.99999998 310.        ] Error: [6.77200547e-03 3.65984044e-16 7.97162516e-19]\n",
      "Pred: [  8.07850675 419.99999999 310.        ] Error: [6.16331053e-03 2.16993162e-16 4.44814077e-19]\n",
      "Pred: [  8.07489544 419.99999999 310.        ] Error: [5.60932753e-03 1.28655697e-16 2.48235690e-19]\n",
      "Pred: [  8.07145025 419.99999999 310.        ] Error: [5.10513873e-03 7.62796254e-17 1.38540810e-19]\n",
      "Pred: [  8.06816354 419.99999999 310.        ] Error: [4.64626844e-03 4.52252801e-17 7.72957663e-20]\n",
      "Pred: [  8.06502802 419.99999999 310.        ] Error: [4.22864325e-03 2.68138390e-17 4.31181110e-20]\n",
      "Pred: [  8.06203673 420.         310.        ] Error: [3.84855588e-03 1.58984147e-17 2.40639797e-20]\n",
      "Pred: [  8.05918304 420.         310.        ] Error: [3.50263228e-03 9.42629223e-18 1.34205014e-20]\n",
      "Pred: [  8.05646062 420.         310.        ] Error: [3.18780168e-03 5.58878416e-18 7.49480242e-21]\n",
      "Pred: [  8.05386343 420.         310.        ] Error: [2.90126932e-03 3.31368740e-18 4.18451285e-21]\n",
      "Pred: [  8.05138571 420.         310.        ] Error: [2.64049163e-03 1.96460877e-18 2.32903364e-21]\n",
      "Pred: [  8.04902197 420.         310.        ] Error: [2.40315368e-03 1.16485826e-18 1.29878988e-21]\n",
      "Pred: [  8.04676696 420.         310.        ] Error: [2.18714861e-03 6.90645405e-19 7.25967310e-22]\n",
      "Pred: [  8.04461568 420.         310.        ] Error: [1.99055895e-03 4.09527311e-19 4.04917835e-22]\n",
      "Pred: [  8.04256336 420.         310.        ] Error: [1.81163955e-03 2.42827790e-19 2.25199922e-22]\n",
      "Pred: [  8.04060544 420.         310.        ] Error: [1.64880214e-03 1.43966989e-19 1.25398642e-22]\n",
      "Pred: [  8.03873759 420.         310.        ] Error: [1.50060121e-03 8.53663317e-20 7.07756412e-23]\n",
      "Pred: [  8.03695566 420.         310.        ] Error: [1.36572117e-03 5.06188135e-20 3.98112982e-23]\n",
      "Pred: [  8.0352557 420.        310.       ] Error: [1.24296469e-03 3.00185912e-20 2.17264158e-23]\n",
      "Pred: [  8.03363394 420.         310.        ] Error: [1.13124205e-03 1.77986294e-20 1.24206339e-23]\n",
      "Pred: [  8.03208678 420.         310.        ] Error: [1.02956149e-03 1.05389143e-20 6.83716475e-24]\n",
      "Pred: [  8.03061079 420.         310.        ] Error: [9.37020382e-04 6.25193770e-21 3.73523745e-24]\n",
      "Pred: [  8.02920269 420.         310.        ] Error: [8.52797242e-04 3.69937142e-21 2.18427381e-24]\n",
      "Pred: [  8.02785937 420.         310.        ] Error: [7.76144415e-04 2.19388978e-21 1.16645391e-24]\n",
      "Pred: [  8.02657784 420.         310.        ] Error: [7.06381450e-04 1.30289024e-21 6.33310156e-25]\n",
      "Pred: [  8.02535526 420.         310.        ] Error: [6.42889060e-04 7.69484765e-22 3.90972086e-25]\n",
      "Pred: [  8.02418892 420.         310.        ] Error: [5.85103620e-04 4.56810493e-22 1.58327539e-25]\n",
      "Pred: [  8.02307623 420.         310.        ] Error: [5.32512166e-04 2.71741756e-22 1.16322274e-25]\n",
      "Pred: [  8.02201472 420.         310.        ] Error: [4.84647843e-04 1.62127400e-22 8.07793567e-26]\n",
      "Pred: [  8.02100204 420.         310.        ] Error: [4.41085756e-04 9.55910595e-23 2.90805684e-26]\n",
      "Pred: [  8.02003595 420.         310.        ] Error: [4.01439204e-04 5.62999804e-23 2.90805684e-26]\n",
      "Pred: [  8.01911429 420.         310.        ] Error: [3.65356242e-04 3.36171371e-23 1.29246971e-26]\n",
      "Pred: [  8.01823504 420.         310.        ] Error: [3.32516562e-04 2.01657586e-23 3.23117427e-27]\n",
      "Pred: [  8.01739622 420.         310.        ] Error: [3.02628643e-04 1.16322274e-23 3.23117427e-27]\n",
      "Pred: [  8.016596 420.       310.      ] Error: [2.75427170e-04 6.54312789e-24 3.23117427e-27]\n",
      "Pred: [  8.01583258 420.         310.        ] Error: [2.50670675e-04 3.95818848e-24 3.23117427e-27]\n",
      "Pred: [  8.01510428 420.         310.        ] Error: [2.28139392e-04 2.35552604e-24 3.23117427e-27]\n",
      "Pred: [  8.01440949 420.         310.        ] Error: [2.07633311e-04 1.42494785e-24 3.23117427e-27]\n",
      "Pred: [  8.01374665 420.         310.        ] Error: [1.88970398e-04 8.27180613e-25 3.23117427e-27]\n",
      "Pred: [  8.0131143 420.        310.       ] Error: [1.71984983e-04 4.65289095e-25 3.23117427e-27]\n",
      "Pred: [  8.01251105 420.         310.        ] Error: [1.56526285e-04 3.23117427e-25 3.23117427e-27]\n",
      "Pred: [  8.01193554 420.         310.        ] Error: [1.42457076e-04 1.58327539e-25 3.23117427e-27]\n",
      "Pred: [  8.0113865 420.        310.       ] Error: [1.29652464e-04 8.07793567e-26 3.23117427e-27]\n",
      "Pred: [  8.01086272 420.         310.        ] Error: [1.17998782e-04 5.16987883e-26 3.23117427e-27]\n",
      "Pred: [  8.01036304 420.         310.        ] Error: [1.07392580e-04 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00988634 420.         310.        ] Error: [9.77397050e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00943157 420.         310.        ] Error: [8.89544693e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00899772 420.         310.        ] Error: [8.09588858e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00858382 420.         310.        ] Error: [7.36819773e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00818896 420.         310.        ] Error: [6.70591465e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00781227 420.         310.        ] Error: [6.10316021e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00745291 420.         310.        ] Error: [5.55458376e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00711007 420.         310.        ] Error: [5.05531555e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00678301 420.         310.        ] Error: [4.60092357e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00647099 420.         310.        ] Error: [4.18737416e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00617333 420.         310.        ] Error: [3.81099622e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00588935 420.         310.        ] Error: [3.46844863e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00561844 420.         310.        ] Error: [3.15669060e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00535999 420.         310.        ] Error: [2.87295462e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00511344 420.         310.        ] Error: [2.61472197e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00487822 420.         310.        ] Error: [2.37970030e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00465382 420.         310.        ] Error: [2.16580332e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00443974 420.         310.        ] Error: [1.97113225e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00423552 420.         310.        ] Error: [1.79395900e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00404068 420.         310.        ] Error: [1.63271079e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00385481 420.         310.        ] Error: [1.48595621e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00367749 420.         310.        ] Error: [1.35239252e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00350832 420.         310.        ] Error: [1.23083407e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00334694 420.         310.        ] Error: [1.12020178e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00319298 420.         310.        ] Error: [1.01951357e-05 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00304611 420.         310.        ] Error: [9.27875610e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00290598 420.         310.        ] Error: [8.44474438e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00277231 420.         310.        ] Error: [7.68569698e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00264478 420.         310.        ] Error: [6.99487579e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00252312 420.         310.        ] Error: [6.36614838e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00240706 420.         310.        ] Error: [5.79393350e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00229633 420.         310.        ] Error: [5.27315158e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0021907 420.        310.       ] Error: [4.79917962e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00208993 420.         310.        ] Error: [4.36781016e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00199379 420.         310.        ] Error: [3.97521391e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00190208 420.         310.        ] Error: [3.61790578e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00181458 420.         310.        ] Error: [3.29271394e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00173111 420.         310.        ] Error: [2.99675164e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00165148 420.         310.        ] Error: [2.72739162e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00157551 420.         310.        ] Error: [2.48224275e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00150304 420.         310.        ] Error: [2.25912884e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0014339 420.        310.       ] Error: [2.05606930e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00136794 420.         310.        ] Error: [1.87126157e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00130502 420.         310.        ] Error: [1.70306510e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00124498 420.         310.        ] Error: [1.54998679e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00118772 420.         310.        ] Error: [1.41066778e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00113308 420.         310.        ] Error: [1.28387132e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00108096 420.         310.        ] Error: [1.16847183e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00103123 420.         310.        ] Error: [1.06344491e-06 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0009838 420.        310.       ] Error: [9.67858224e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00093854 420.         310.        ] Error: [8.80863256e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00089537 420.         310.        ] Error: [8.01687743e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00085418 420.         310.        ] Error: [7.29628842e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00081489 420.         310.        ] Error: [6.64046883e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00077741 420.         310.        ] Error: [6.04359693e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00074165 420.         310.        ] Error: [5.50037426e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00070753 420.         310.        ] Error: [5.00597862e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00067498 420.         310.        ] Error: [4.55602124e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00064393 420.         310.        ] Error: [4.14650783e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00061431 420.         310.        ] Error: [3.77380312e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00058605 420.         310.        ] Error: [3.43459860e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0005591 420.        310.       ] Error: [3.12588314e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00053338 420.         310.        ] Error: [2.84491626e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00050884 420.         310.        ] Error: [2.58920380e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00048544 420.         310.        ] Error: [2.35647581e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00046311 420.         310.        ] Error: [2.14466634e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0004418 420.        310.       ] Error: [1.95189515e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00042148 420.         310.        ] Error: [1.77645101e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00040209 420.         310.        ] Error: [1.61677648e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0003836 420.        310.       ] Error: [1.47145415e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00036595 420.         310.        ] Error: [1.33919396e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00034912 420.         310.        ] Error: [1.21882185e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00033306 420.         310.        ] Error: [1.10926927e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00031774 420.         310.        ] Error: [1.00956371e-07 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00030312 420.         310.        ] Error: [9.18820085e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00028918 420.         310.        ] Error: [8.36232860e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00027587 420.         310.        ] Error: [7.61068906e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00026318 420.         310.        ] Error: [6.92660988e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00025108 420.         310.        ] Error: [6.30401848e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00023953 420.         310.        ] Error: [5.73738808e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00022851 420.         310.        ] Error: [5.22168869e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.000218 420.       310.      ] Error: [4.75234243e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00020797 420.         310.        ] Error: [4.32518288e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0001984 420.        310.       ] Error: [3.93641814e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00018928 420.         310.        ] Error: [3.58259713e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00018057 420.         310.        ] Error: [3.26057897e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00017226 420.         310.        ] Error: [2.96750509e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00016434 420.         310.        ] Error: [2.70077386e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00015678 420.         310.        ] Error: [2.45801751e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00014957 420.         310.        ] Error: [2.23708106e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00014269 420.         310.        ] Error: [2.03600327e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00013612 420.         310.        ] Error: [1.85299915e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00012986 420.         310.        ] Error: [1.68644417e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00012389 420.         310.        ] Error: [1.53485983e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00011819 420.         310.        ] Error: [1.39690048e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00011275 420.         310.        ] Error: [1.27134148e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00010757 420.         310.        ] Error: [1.15706822e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00010262 420.         310.        ] Error: [1.05306630e-08 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000979 420.        310.       ] Error: [9.58412492e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000934 420.        310.       ] Error: [8.72266544e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000891 420.        310.       ] Error: [7.93863738e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.000085 420.       310.      ] Error: [7.22508089e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00008109 420.         310.        ] Error: [6.57566172e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00007736 420.         310.        ] Error: [5.98461494e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000738 420.        310.       ] Error: [5.44669382e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00007041 420.         310.        ] Error: [4.95712319e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00006717 420.         310.        ] Error: [4.51155713e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00006408 420.         310.        ] Error: [4.10604033e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00006113 420.         310.        ] Error: [3.73697300e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00005832 420.         310.        ] Error: [3.40107892e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00005564 420.         310.        ] Error: [3.09537634e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00005308 420.         310.        ] Error: [2.81715153e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00005064 420.         310.        ] Error: [2.56393468e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00004831 420.         310.        ] Error: [2.33347798e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00004608 420.         310.        ] Error: [2.12373564e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00004396 420.         310.        ] Error: [1.93284579e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00004194 420.         310.        ] Error: [1.75911388e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00004001 420.         310.        ] Error: [1.60099769e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00003817 420.         310.        ] Error: [1.45709361e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00003642 420.         310.        ] Error: [1.32612421e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00003474 420.         310.        ] Error: [1.20692686e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00003314 420.         310.        ] Error: [1.09844345e-09 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00003162 420.         310.        ] Error: [9.99710956e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00003016 420.         310.        ] Error: [9.09852936e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00002878 420.         310.        ] Error: [8.28071715e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00002745 420.         310.        ] Error: [7.53641317e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00002619 420.         310.        ] Error: [6.85901021e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00002498 420.         310.        ] Error: [6.24249493e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00002384 420.         310.        ] Error: [5.68139452e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00002274 420.         310.        ] Error: [5.17072805e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00002169 420.         310.        ] Error: [4.70596233e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000207 420.        310.       ] Error: [4.28297162e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00001974 420.         310.        ] Error: [3.89800099e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00001884 420.         310.        ] Error: [3.54763307e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00001797 420.         310.        ] Error: [3.22875762e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00001714 420.         310.        ] Error: [2.93854397e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00001635 420.         310.        ] Error: [2.67441589e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000156 420.        310.       ] Error: [2.43402869e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00001488 420.         310.        ] Error: [2.21524845e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000142 420.        310.       ] Error: [2.01613306e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00001355 420.         310.        ] Error: [1.83491496e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00001292 420.         310.        ] Error: [1.66998546e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00001233 420.         310.        ] Error: [1.51988049e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00001176 420.         310.        ] Error: [1.38326755e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00001122 420.         310.        ] Error: [1.25893393e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000107 420.        310.       ] Error: [1.14577591e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00001021 420.         310.        ] Error: [1.04278899e-10 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000974 420.         310.        ] Error: [9.49058945e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000929 420.         310.        ] Error: [8.63753731e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000887 420.         310.        ] Error: [7.86116090e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000846 420.         310.        ] Error: [7.15456832e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000807 420.         310.        ] Error: [6.51148710e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000077 420.        310.       ] Error: [5.92620859e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000734 420.         310.        ] Error: [5.39353726e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000701 420.         310.        ] Error: [4.90874456e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000668 420.         310.        ] Error: [4.46752696e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000638 420.         310.        ] Error: [4.06596777e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000608 420.         310.        ] Error: [3.70050232e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000058 420.        310.       ] Error: [3.36788637e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000554 420.         310.        ] Error: [3.06516727e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000528 420.         310.        ] Error: [2.78965778e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000504 420.         310.        ] Error: [2.53891218e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000481 420.         310.        ] Error: [2.31070459e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000459 420.         310.        ] Error: [2.10300922e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000437 420.         310.        ] Error: [1.91398234e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000417 420.         310.        ] Error: [1.74194595e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000398 420.         310.        ] Error: [1.58537288e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000038 420.        310.       ] Error: [1.44287323e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000362 420.         310.        ] Error: [1.31318201e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000346 420.         310.        ] Error: [1.19514796e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000033 420.        310.       ] Error: [1.08772328e-11 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000315 420.         310.        ] Error: [9.89954360e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.000003 420.       310.      ] Error: [9.00973302e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000286 420.         310.        ] Error: [8.19990217e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000273 420.         310.        ] Error: [7.46286217e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000261 420.         310.        ] Error: [6.79207026e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000249 420.         310.        ] Error: [6.18157182e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000237 420.         310.        ] Error: [5.62594742e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000226 420.         310.        ] Error: [5.12026476e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000216 420.         310.        ] Error: [4.66003488e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000206 420.         310.        ] Error: [4.24117230e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000196 420.         310.        ] Error: [3.85995877e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000187 420.         310.        ] Error: [3.51301024e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000179 420.         310.        ] Error: [3.19724683e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000171 420.         310.        ] Error: [2.90986549e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000163 420.         310.        ] Error: [2.64831514e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000155 420.         310.        ] Error: [2.41027398e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000148 420.         310.        ] Error: [2.19362892e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000141 420.         310.        ] Error: [1.99645678e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000135 420.         310.        ] Error: [1.81700726e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000129 420.         310.        ] Error: [1.65368738e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000123 420.         310.        ] Error: [1.50504734e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000117 420.         310.        ] Error: [1.36976767e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000112 420.         310.        ] Error: [1.24664747e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000107 420.         310.        ] Error: [1.13459381e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000102 420.         310.        ] Error: [1.03261197e-12 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000097 420.         310.        ] Error: [9.39796680e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000092 420.         310.        ] Error: [8.55323995e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000088 420.         310.        ] Error: [7.78444053e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000084 420.         310.        ] Error: [7.08474387e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000008 420.        310.       ] Error: [6.44793876e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000077 420.         310.        ] Error: [5.86837221e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000073 420.         310.        ] Error: [5.34089946e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000007 420.        310.       ] Error: [4.86083804e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000067 420.         310.        ] Error: [4.42392649e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000063 420.         310.        ] Error: [4.02628628e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000061 420.         310.        ] Error: [3.66438757e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000058 420.         310.        ] Error: [3.33501775e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000055 420.         310.        ] Error: [3.03525300e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000053 420.         310.        ] Error: [2.76243233e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000005 420.        310.       ] Error: [2.51413386e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000048 420.         310.        ] Error: [2.28815345e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000046 420.         310.        ] Error: [2.08248506e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000044 420.         310.        ] Error: [1.89530298e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000042 420.         310.        ] Error: [1.72494557e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000004 420.        310.       ] Error: [1.56990057e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000038 420.         310.        ] Error: [1.42879162e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000036 420.         310.        ] Error: [1.30036612e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000034 420.         310.        ] Error: [1.18348400e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000033 420.         310.        ] Error: [1.07710772e-13 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000031 420.         310.        ] Error: [9.80292970e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000003 420.        310.       ] Error: [8.92180320e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000028 420.         310.        ] Error: [8.11987588e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000027 420.         310.        ] Error: [7.39002892e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000026 420.         310.        ] Error: [6.72578364e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000025 420.         310.        ] Error: [6.12124324e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000024 420.         310.        ] Error: [5.57104146e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000023 420.         310.        ] Error: [5.07029400e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000021 420.         310.        ] Error: [4.61455565e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000002 420.        310.       ] Error: [4.19978098e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000002 420.        310.       ] Error: [3.82228783e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000019 420.         310.        ] Error: [3.47872531e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000018 420.         310.        ] Error: [3.16604357e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000017 420.         310.        ] Error: [2.88146689e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000016 420.         310.        ] Error: [2.62246911e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000015 420.         310.        ] Error: [2.38675107e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000015 420.         310.        ] Error: [2.17222032e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000014 420.         310.        ] Error: [1.97697245e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000013 420.         310.        ] Error: [1.79927424e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000013 420.         310.        ] Error: [1.63754828e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000012 420.         310.        ] Error: [1.49035892e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000012 420.         310.        ] Error: [1.35639947e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000011 420.         310.        ] Error: [1.23448087e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000011 420.         310.        ] Error: [1.12352080e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000001 420.        310.       ] Error: [1.02253423e-14 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.0000001 420.        310.       ] Error: [9.30624772e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000009 420.         310.        ] Error: [8.46976507e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000009 420.         310.        ] Error: [7.70846853e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000008 420.         310.        ] Error: [7.01560069e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000008 420.         310.        ] Error: [6.38501029e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000008 420.         310.        ] Error: [5.81110012e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000007 420.         310.        ] Error: [5.28877516e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000007 420.         310.        ] Error: [4.81339885e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000007 420.         310.        ] Error: [4.38075129e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000006 420.         310.        ] Error: [3.98699181e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000006 420.         310.        ] Error: [3.62862514e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000006 420.         310.        ] Error: [3.30246977e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000005 420.         310.        ] Error: [3.00563061e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000005 420.         310.        ] Error: [2.73547236e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000005 420.         310.        ] Error: [2.48959719e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000005 420.         310.        ] Error: [2.26582227e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000005 420.         310.        ] Error: [2.06216113e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000004 420.         310.        ] Error: [1.87680571e-15 1.29246971e-26 3.23117427e-27]\n",
      "Pred: [  8.00000004 420.         310.        ] Error: [1.70811096e-15 1.29246971e-26 3.23117427e-27]\n"
     ]
    }
   ],
   "source": [
    "input, goal_pred = np.array([2, 10, 11]), np.array([8, 420, 310]);\n",
    "weights = np.random.rand(3, 3);\n",
    "alpha = .001;\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    for i in range(iteration):\n",
    "        pred = input.dot(weights);\n",
    "        msquared_error = (pred - goal_pred) ** 2;\n",
    "        delta = pred - goal_pred;\n",
    "        weight_delta = input * delta;\n",
    "        weights -= weight_delta * alpha;\n",
    "        print('Pred: ' + str(pred) + ' Error: ' + str(msquared_error));\n",
    "        if(not np.any(msquared_error)): break;\n",
    "            \n",
    "neural_network(input, weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The takeaway:\n",
    "    Dot product is a loose measurement of similarity between \n",
    "    two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the book will be spent exploring different types\n",
    "of weight combinations and error functions for which \n",
    "Gradient Descent is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
