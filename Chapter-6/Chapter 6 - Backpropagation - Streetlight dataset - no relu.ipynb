{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix A * 10 == Matrix B - These matrices are scalar multiple of each other;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn one streetlight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "\n",
    "alpha = .1\n",
    "iterations = 40\n",
    "#The streetlight pattern;\n",
    "streetlights = np.array([ [1, 0, 1],\n",
    "                         [0, 1, 1], \n",
    "                         [0, 0, 1],\n",
    "                         [1, 1, 1],\n",
    "                         [0, 1, 1],\n",
    "                         [1, 0, 1] ]);\n",
    "\n",
    "streetlight = streetlights[0]\n",
    "#1 means walk, 0 means stop;\n",
    "# lossless representation: \n",
    "# >can perfectly convert back and forth between \n",
    "# >our stop/walk notes and our matrix;\n",
    "goal_pred = np.array([ 0, 1, 0, 1, 1, 0 ]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent - Update weights for each training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.17022005e-01 7.20324493e-01 1.14374817e-04]\n",
      "Error: 0.507470215713197\n",
      "Pred: [0.4171363795199189, 0.6787252303075111, -0.009471786165398115, 1.1192357296131308, 0.7200802169399225, 0.3709285915851054]\n",
      "-----------------------------\n",
      "Error: 0.31560515640555536\n",
      "Pred: [0.29674287326808435, 0.709297027066619, -0.030153051396144506, 1.0670705740968598, 0.7570388119735377, 0.2803619050306943]\n",
      "-----------------------------\n",
      "Error: 0.2020506734994488\n",
      "Pred: [0.22428952402455543, 0.7551659066733052, -0.03553041843642529, 1.0471312141710258, 0.7982595243480816, 0.21821587512694304]\n",
      "-----------------------------\n",
      "Error: 0.13022294889293376\n",
      "Pred: [0.17457270010155443, 0.7993287619556154, -0.03572818416310479, 1.0384892960295895, 0.835337968774885, 0.17206644621858605]\n",
      "-----------------------------\n",
      "Error: 0.0841700986788962\n",
      "Pred: [0.13765315697486885, 0.8372984147005624, -0.034239894016643485, 1.0338952992885966, 0.8664836613043949, 0.1363892475233444]\n",
      "-----------------------------\n",
      "Error: 0.054550173924685724\n",
      "Pred: [0.1091113980186755, 0.868636864489314, -0.032267551677411674, 1.0308292304026132, 0.8919704006786696, 0.10828930098536056]\n",
      "-----------------------------\n",
      "Error: 0.03547278955837782\n",
      "Pred: [0.08663144078828845, 0.8940842463655709, -0.030221258431720785, 1.0284075093614231, 0.9126080210633442, 0.08597654985862671]\n",
      "-----------------------------\n",
      "Error: 0.0231688404771947\n",
      "Pred: [0.06878123988690138, 0.9146106378761225, -0.028237528393190492, 1.0263137196553165, 0.9292495192091538, 0.06819998510924923]\n",
      "-----------------------------\n",
      "Error: 0.01521967241494394\n",
      "Pred: [0.05455998808739938, 0.9311236180476582, -0.026358458564749148, 1.0244288275245044, 0.9426489747897007, 0.05402081153775765]\n",
      "-----------------------------\n",
      "Error: 0.010072180398149621\n",
      "Pred: [0.04321664923020613, 0.9443954337549642, -0.02459368239198753, 1.0227031736438261, 0.9534350805144047, 0.042709001467661545]\n",
      "-----------------------------\n",
      "Error: 0.006728737451947981\n",
      "Pred: [0.03416720117412923, 0.9550604441473448, -0.02294180424752542, 1.0211120565147227, 0.9621201244396839, 0.03368747320240852]\n",
      "-----------------------------\n",
      "Error: 0.004548310346185544\n",
      "Pred: [0.026949978561926817, 0.9636323543753135, -0.02139782253217842, 1.019640235697657, 0.9691176186139373, 0.026496720664302805]\n",
      "-----------------------------\n",
      "Error: 0.0031188189273992886\n",
      "Pred: [0.02119737653144224, 0.9705246851715753, -0.019955703946852046, 1.0182764551867938, 0.9747600274945867, 0.020769709315864048]\n",
      "-----------------------------\n",
      "Error: 0.002175207599917957\n",
      "Pred: [0.01661576745269124, 0.9760694743188137, -0.018609276929041792, 1.0170114506072687, 0.9793142170265015, 0.01621288239907191]\n",
      "-----------------------------\n",
      "Error: 0.0015468543782868858\n",
      "Pred: [0.01297030591925753, 0.9805330547893683, -0.0173525403102844, 1.0158371774292767, 0.9829942623766678, 0.012591331563975523]\n",
      "-----------------------------\n",
      "Error: 0.0011238094676164728\n",
      "Pred: [0.010073065251180416, 0.9841289702198186, -0.01617976696334786, 1.0147464750145, 0.9859718578692896, 0.009717051085468272]\n",
      "-----------------------------\n",
      "Error: 0.0008351194146670225\n",
      "Pred: [0.007773640868374617, 0.9870284171000475, -0.015085534460781093, 1.013732892571592, 0.9883847086117976, 0.0074395750552748805]\n",
      "-----------------------------\n",
      "Error: 0.0006349085581748461\n",
      "Pred: [0.005951660044219905, 0.9893686433794887, -0.0140647289809403, 1.0127905802800523, 0.9903432715456745, 0.005638493384943174]\n",
      "-----------------------------\n",
      "Error: 0.0004934398676414228\n",
      "Pred: [0.004510794707954539, 0.9912596884272499, -0.013112538917433705, 1.0119142104746157, 0.9919361625386202, 0.004217462466596855]\n",
      "-----------------------------\n",
      "Error: 0.0003913722630741851\n",
      "Pred: [0.003373969973277484, 0.9927897867869088, -0.012224444249692233, 1.0110989154041197, 0.9932344907736723, 0.0030994095667091683]\n",
      "-----------------------------\n",
      "Error: 0.000316070089767351\n",
      "Pred: [0.0024795276533673347, 0.9940296988969303, -0.011396204054202885, 1.010340235810168, 0.9942953323609307, 0.0022226922402944424]\n",
      "-----------------------------\n",
      "Error: 0.0002592302587383081\n",
      "Pred: [0.0017781537922355525, 0.9950361812854914, -0.010623843197694618, 1.0096340774510966, 0.9951645138579432, 0.0015380223489951227]\n",
      "-----------------------------\n",
      "Error: 0.00021535559602334046\n",
      "Pred: [0.0012304178791960971, 0.9958547670635355, -0.009903638738001808, 1.008976673859634, 0.995878842752702, 0.0010060024236065168]\n",
      "-----------------------------\n",
      "Error: 0.0001807717607835366\n",
      "Pred: [0.0008048019388852135, 0.9965219937659124, -0.00923210633827564, 1.00836455415935, 0.9964678948146876, 0.0005951524950057359]\n",
      "-----------------------------\n",
      "Error: 0.00015299344281742187\n",
      "Pred: [0.0004761219960045873, 0.9970671884026491, -0.008605986891217773, 1.0077945150589978, 0.9969554463994414, 0.00028032979391684476]\n",
      "-----------------------------\n",
      "Error: 0.0001303150943341567\n",
      "Pred: [0.00022426383513347511, 0.997513897756648, -0.008022233486509748, 1.007263596332921, 0.9973606222873851, 4.146314577023382e-05]\n",
      "-----------------------------\n",
      "Error: 0.00011154608936902011\n",
      "Pred: [3.317051661618723e-05, 0.9978810344636694, -0.0074779988124949685, 1.006769059231606, 0.9976988156058638, -0.00013746055873209082]\n",
      "-----------------------------\n",
      "Error: 9.583885814117396e-05\n",
      "Pred: [-0.00010996844698567265, 0.9981837953852628, -0.006970623052946967, 1.006308367370337, 0.9979824251394375, -0.0002692079788312743]\n",
      "-----------------------------\n",
      "Error: 8.257693836178848e-05\n",
      "Pred: [-0.00021536638306501963, 0.9984343975477395, -0.006497622317214045, 1.0058791697259013, 0.9982214463247328, -0.00036394920715810113]\n",
      "-----------------------------\n",
      "Error: 7.130169547005836e-05\n",
      "Pred: [-0.0002911593657264804, 0.9986426679170746, -0.006056677624975044, 1.0054792854368435, 0.9984239450087886, -0.0004297781100386991]\n",
      "-----------------------------\n",
      "Error: 6.166404725257403e-05\n",
      "Pred: [-0.00034382248803095997, 0.998816516066838, -0.005645624453917583, 1.0051066901556711, 0.9985964372677278, -0.0004731289096238135]\n",
      "-----------------------------\n",
      "Error: 5.339239880959339e-05\n",
      "Pred: [-0.0003785031276990515, 0.9989623130179145, -0.0052624428489248835, 1.0047595037441983, 0.9987441939503845, -0.0004991096629363137]\n",
      "-----------------------------\n",
      "Error: 4.627112276980291e-05\n",
      "Pred: [-0.0003992877303490511, 0.9990851948996362, -0.00490524808412576, 1.0044359791380042, 0.9988714849005206, -0.0005117691834831822]\n",
      "-----------------------------\n",
      "Error: 4.012593049417442e-05\n",
      "Pred: [-0.00040941534678654544, 0.9991893063734435, -0.004572281863883038, 1.0041344922342526, 0.9989817748382924, -0.0005143106590650356]\n",
      "-----------------------------\n",
      "Error: 3.4813772927889044e-05\n",
      "Pred: [-0.0004114485272520285, 0.9992779957892658, -0.004261904045044105, 1.003853532680233, 0.9990778804998703, -0.0005092625822574089]\n",
      "-----------------------------\n",
      "Error: 3.021574162357085e-05\n",
      "Pred: [-0.0004074100658059271, 0.9993539716647024, -0.003972584860213928, 1.0035916954588824, 0.999162096726007, -0.0004986154974707642]\n",
      "-----------------------------\n",
      "Error: 2.623197615894591e-05\n",
      "Pred: [-0.0003988923979766114, 0.9994194281703503, -0.003702897620171768, 1.0033476731830528, 0.999236297661687, -0.0004839313761784139]\n",
      "-----------------------------\n",
      "Error: 2.2777929564133714e-05\n",
      "Pred: [-0.00038714510094273093, 0.9994761457770617, -0.0034515118726226265, 1.003120249023074, 0.9993020180042969, -0.000466431076242565]\n",
      "-----------------------------\n",
      "Error: 1.978156642384192e-05\n",
      "Pred: [-0.00037314486099405183, 0.9995255719971611, -0.0032171869940898906, 1.0029082902027164, 0.9993605182565946, -0.00044706425530510307]\n",
      "-----------------------------\n",
      "Error: 1.7181212673539692e-05\n",
      "Pred: [-0.0003576514042440823, 0.9995688861712305, -0.002998766191780134, 1.0027107420074244, 0.9994128371546777, -0.00042656523829294685]\n",
      "-----------------------------\n",
      "Weight: [ 0.00252734  1.00244152 -0.00286859]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "weight = np.random.rand(3)\n",
    "print(weight)\n",
    "\n",
    "def neural_network(input, weight):\n",
    "    for iteration in range(iterations):\n",
    "        error = 0\n",
    "        preds = []\n",
    "        for index, observation in enumerate(input):\n",
    "            pred = observation.dot(weight)\n",
    "            preds.append(pred)\n",
    "            delta = pred - goal_pred[index]\n",
    "            error += delta ** 2\n",
    "            weight_delta = delta * observation\n",
    "            weight -= weight_delta * alpha\n",
    "        print('Error: ' + str(error))\n",
    "        print('Pred: ' + str(preds) + '\\n-----------------------------')\n",
    "    print('Weight: ' + str(weight))\n",
    "            \n",
    "neural_network(streetlights, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full gradient descent - Update weights for the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.17022005e-01 7.20324493e-01 1.14374817e-04]\n",
      "Error: 0.08720164594585363\n",
      "Pred: [4.17136380e-01 7.20438868e-01 1.14374817e-04 1.13746087e+00\n",
      " 7.20438868e-01 4.17136380e-01]\n",
      "-----------------------------\n",
      "Error: 0.05233963816862639\n",
      "Pred: [ 0.27869044  0.72133243 -0.0411582   1.04118107  0.72133243  0.27869044]\n",
      "-----------------------------\n",
      "Error: 0.03361027010807473\n",
      "Pred: [ 0.21882738  0.77294098 -0.04116506  1.03293342  0.77294098  0.21882738]\n",
      "-----------------------------\n",
      "Error: 0.021622416242555387\n",
      "Pred: [ 0.17423806  0.81752893 -0.03869557  1.03046256  0.81752893  0.17423806]\n",
      "-----------------------------\n",
      "Error: 0.01393636211776795\n",
      "Pred: [ 0.13881409  0.85344679 -0.03622567  1.02848655  0.85344679  0.13881409]\n",
      "-----------------------------\n",
      "Error: 0.00900512295505589\n",
      "Pred: [ 0.11052435  0.88223051 -0.03390393  1.0266588   0.88223051  0.11052435]\n",
      "-----------------------------\n",
      "Error: 0.00583847489035693\n",
      "Pred: [ 0.08792714  0.90529207 -0.03173039  1.02494961  0.90529207  0.08792714]\n",
      "-----------------------------\n",
      "Error: 0.003802487520736191\n",
      "Pred: [ 0.06988099  0.92377293 -0.02969616  1.02335008  0.92377293  0.06988099]\n",
      "-----------------------------\n",
      "Error: 0.0024912812669531533\n",
      "Pred: [ 0.05547361  0.93858716 -0.02779233  1.0218531   0.93858716  0.05547361]\n",
      "-----------------------------\n",
      "Error: 0.0016449494451628976\n",
      "Pred: [ 0.04397535  0.95046619 -0.02601056  1.0204521   0.95046619  0.04397535]\n",
      "-----------------------------\n",
      "Error: 0.0010970258657863356\n",
      "Pred: [ 0.03480261  0.95999528 -0.02434302  1.01914091  0.95999528  0.03480261]\n",
      "-----------------------------\n",
      "Error: 0.0007408618825159619\n",
      "Pred: [ 0.02748863  0.96764277 -0.02278239  1.01791378  0.96764277  0.02748863]\n",
      "-----------------------------\n",
      "Error: 0.0005081057642182188\n",
      "Pred: [ 0.02166011  0.97378342 -0.02132181  1.01676533  0.97378342  0.02166011]\n",
      "-----------------------------\n",
      "Error: 0.0003549277941964201\n",
      "Pred: [ 0.01701849  0.97871714 -0.01995486  1.0156905   0.97871714  0.01701849]\n",
      "-----------------------------\n",
      "Error: 0.0002532028457969883\n",
      "Pred: [ 0.01332505  0.98268397 -0.01867556  1.01468458  0.98268397  0.01332505]\n",
      "-----------------------------\n",
      "Error: 0.0001848659275416496\n",
      "Pred: [ 0.01038888  0.98587601 -0.01747826  1.01374315  0.98587601  0.01038888]\n",
      "-----------------------------\n",
      "Error: 0.00013829859050340341\n",
      "Pred: [ 0.00805732  0.98844703 -0.01635773  1.01286208  0.98844703  0.00805732]\n",
      "-----------------------------\n",
      "Error: 0.00010601522919128271\n",
      "Pred: [ 0.00620834  0.99052011 -0.01530904  1.01203749  0.99052011  0.00620834]\n",
      "-----------------------------\n",
      "Error: 8.318143837047652e-05\n",
      "Pred: [ 0.00474439  0.9921938  -0.01432757  1.01126576  0.9921938   0.00474439]\n",
      "-----------------------------\n",
      "Error: 6.666499437002532e-05\n",
      "Pred: [ 0.00358748  0.99354701 -0.01340903  1.01054351  0.99354701  0.00358748]\n",
      "-----------------------------\n",
      "Error: 5.4427811262107885e-05\n",
      "Pred: [ 0.00267528  0.99464291 -0.01254937  1.00986757  0.99464291  0.00267528]\n",
      "-----------------------------\n",
      "Error: 4.5136204343480054e-05\n",
      "Pred: [ 0.00195801  0.99553211 -0.01174483  1.00923496  0.99553211  0.00195801]\n",
      "-----------------------------\n",
      "Error: 3.791094332575468e-05\n",
      "Pred: [ 0.00139588  0.99625516 -0.01099187  1.0086429   0.99625516  0.00139588]\n",
      "-----------------------------\n",
      "Error: 3.216683491041119e-05\n",
      "Pred: [ 9.57101044e-04  9.96844525e-01 -1.02871807e-02  1.00808881e+00\n",
      "  9.96844525e-01  9.57101044e-04]\n",
      "-----------------------------\n",
      "Error: 2.7509660201594498e-05\n",
      "Pred: [ 6.16312421e-04  9.97326251e-01 -9.62766843e-03  1.00757023e+00\n",
      "  9.97326251e-01  6.16312421e-04]\n",
      "-----------------------------\n",
      "Error: 2.3669868292625154e-05\n",
      "Pred: [ 3.53257547e-04  9.97721209e-01 -9.01043759e-03  1.00708490e+00\n",
      "  9.97721209e-01  3.53257547e-04]\n",
      "-----------------------------\n",
      "Error: 2.04598367901599e-05\n",
      "Pred: [ 1.51775753e-04  9.98046137e-01 -8.43277749e-03  1.00663069e+00\n",
      "  9.98046137e-01  1.51775753e-04]\n",
      "-----------------------------\n",
      "Error: 1.7746252829887945e-05\n",
      "Pred: [-1.02213892e-06  9.98314467e-01 -7.89215123e-03  1.00620560e+00\n",
      "  9.98314467e-01 -1.02213892e-06]\n",
      "-----------------------------\n",
      "Error: 1.5432204185312762e-05\n",
      "Pred: [-1.15410635e-04  9.98536980e-01 -7.38618458e-03  1.00580775e+00\n",
      "  9.98536980e-01 -1.15410635e-04]\n",
      "-----------------------------\n",
      "Error: 1.3445513721659644e-05\n",
      "Pred: [-1.99574868e-04  9.98722338e-01 -6.91265551e-03  1.00543542e+00\n",
      "  9.98722338e-01 -1.99574868e-04]\n",
      "-----------------------------\n",
      "Error: 1.17310948912075e-05\n",
      "Pred: [-2.60030679e-04  9.98877500e-01 -6.46948443e-03  1.00508695e+00\n",
      "  9.98877500e-01 -2.60030679e-04]\n",
      "-----------------------------\n",
      "Error: 1.0245902855641763e-05\n",
      "Pred: [-3.01960546e-04  9.99008064e-01 -6.05472510e-03  1.00476083e+00\n",
      "  9.99008064e-01 -3.01960546e-04]\n",
      "-----------------------------\n",
      "Error: 8.955566219853891e-06\n",
      "Pred: [-3.29482192e-04  9.99118537e-01 -5.66655604e-03  1.00445561e+00\n",
      "  9.99118537e-01 -3.29482192e-04]\n",
      "-----------------------------\n",
      "Error: 7.832111358229167e-06\n",
      "Pred: [-3.45863349e-04  9.99212552e-01 -5.30327253e-03  1.00416996e+00\n",
      "  9.99212552e-01 -3.45863349e-04]\n",
      "-----------------------------\n",
      "Error: 6.8524008912745045e-06\n",
      "Pred: [-3.53693449e-04  9.99293039e-01 -4.96327917e-03  1.00390262e+00\n",
      "  9.99293039e-01 -3.53693449e-04]\n",
      "-----------------------------\n",
      "Error: 5.997042261463734e-06\n",
      "Pred: [-3.55020874e-04  9.99362365e-01 -4.64508282e-03  1.00365243e+00\n",
      "  9.99362365e-01 -3.55020874e-04]\n",
      "-----------------------------\n",
      "Error: 5.249608596819737e-06\n",
      "Pred: [-3.51462647e-04  9.99422446e-01 -4.34728607e-03  1.00341827e+00\n",
      "  9.99422446e-01 -3.51462647e-04]\n",
      "-----------------------------\n",
      "Error: 4.596069445022058e-06\n",
      "Pred: [-3.44292099e-04  9.99474835e-01 -4.06858110e-03  1.00319912e+00\n",
      "  9.99474835e-01 -3.44292099e-04]\n",
      "-----------------------------\n",
      "Error: 4.024364589126403e-06\n",
      "Pred: [-3.34508904e-04  9.99520793e-01 -3.80774393e-03  1.00299403e+00\n",
      "  9.99520793e-01 -3.34508904e-04]\n",
      "-----------------------------\n",
      "Error: 3.524077112240667e-06\n",
      "Pred: [-3.22895029e-04  9.99561346e-01 -3.56362907e-03  1.00280208e+00\n",
      "  9.99561346e-01 -3.22895029e-04]\n",
      "-----------------------------\n",
      "Weight: [ 0.00302511  1.0029325  -0.00333516]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "weight = np.random.rand(3)\n",
    "print(weight)\n",
    "\n",
    "def neural_network(input, weight):\n",
    "    for i in range(iterations):  \n",
    "        pred = input.dot(weight);\n",
    "        msquared_error = np.sum((pred - goal_pred) ** 2) / len(input);\n",
    "        delta = pred - goal_pred;\n",
    "        weight_delta = delta.dot(input);\n",
    "        weight -= weight_delta * alpha;\n",
    "        print('Error: ' + str(msquared_error)); \n",
    "        print('Pred: ' + str(pred) + '\\n-----------------------------')\n",
    "    print('Weight: ' + str(weight))\n",
    "\n",
    "    \n",
    "neural_network(streetlights, weight);\n",
    "# random_v3c = np.random.rand(6);\n",
    "# print('rand: ' + str(random_v3c) + ' goal: ' + str(goal_pred.shape));\n",
    "# sub = random_v3c - goal_pred;\n",
    "# print(sub);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if our input node does not have any correlation with our output node? What does it do to our linear network? let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .5\n",
    "iterations = 300\n",
    "#The streetlight pattern;\n",
    "streetlights = np.array([ [1, 0, 1],\n",
    "                          [0, 1, 1],\n",
    "                          [0, 0, 1],\n",
    "                          [1, 1, 1] ]);\n",
    "\n",
    "#1 means walk, 0 means stop;\n",
    "# lossless representation: \n",
    "# >can perfectly convert back and forth between \n",
    "# >our stop/walk notes and our matrix;\n",
    "goal_pred = np.array([ 1, 1, 0, 0 ]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.17022005e-01 7.20324493e-01 1.14374817e-04]\n",
      "Error: 4.0\n",
      "Pred: [0.0, 0.0, 1.0, 1.0]\n",
      "-----------------------------\n",
      "Error: 4.0\n",
      "Pred: [0.0, 0.0, 1.0, 1.0]\n",
      "-----------------------------\n",
      "Error: 4.0\n",
      "Pred: [0.0, 0.0, 1.0, 1.0]\n",
      "-----------------------------\n",
      "Error: 4.0\n",
      "Pred: [0.0, 0.0, 1.0, 1.0]\n",
      "-----------------------------\n",
      "Weight: [ 0.  -0.5  0. ]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "weight = np.random.rand(3)\n",
    "print(weight)\n",
    "\n",
    "def neural_network(input, weight):\n",
    "    for iteration in range(iterations):\n",
    "        error = 0\n",
    "        preds = []\n",
    "        for index, observation in enumerate(input):\n",
    "            pred = observation.dot(weight)\n",
    "            preds.append(pred)\n",
    "            delta = pred - goal_pred[index]\n",
    "            error += delta ** 2\n",
    "            weight_delta = delta * observation\n",
    "            weight -= weight_delta * alpha\n",
    "        if(iteration > 295):\n",
    "                print('Error: ' + str(error))\n",
    "                print('Pred: ' + str(preds) + '\\n-----------------------------')\n",
    "            \n",
    "    print('Weight: ' + str(weight))\n",
    "            \n",
    "neural_network(streetlights, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well it predicted the exact opposite, how nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add nonlinearity and hidden layers to the mix to see if we can correlate.\n",
    "\n",
    "Weighted sums are performed in the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [[1.41420584]]\n",
      "Pred: [array([[0.39194327]]), array([[0.02098811]]), array([[0.29327219]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Error: [[1.21271615]]\n",
      "Pred: [array([[0.53634943]]), array([[0.06741661]]), array([[0.35781632]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Error: [[1.05656376]]\n",
      "Pred: [array([[0.67382026]]), array([[0.10937117]]), array([[0.39543008]]), array([[0.02420473]])]\n",
      "-----------------------------\n",
      "Error: [[0.94916648]]\n",
      "Pred: [array([[0.76081998]]), array([[0.15331219]]), array([[0.41249809]]), array([[0.07017463]])]\n",
      "-----------------------------\n",
      "Error: [[0.86196212]]\n",
      "Pred: [array([[0.81109594]]), array([[0.20020963]]), array([[0.41573064]]), array([[0.11739149]])]\n",
      "-----------------------------\n",
      "Error: [[0.78450684]]\n",
      "Pred: [array([[0.83854849]]), array([[0.24915731]]), array([[0.40940342]]), array([[0.16451243]])]\n",
      "-----------------------------\n",
      "Error: [[0.71464898]]\n",
      "Pred: [array([[0.85254688]]), array([[0.29776432]]), array([[0.39521967]]), array([[0.20874153]])]\n",
      "-----------------------------\n",
      "Error: [[0.67286635]]\n",
      "Pred: [array([[0.85894779]]), array([[0.34351082]]), array([[0.40318833]]), array([[0.24378628]])]\n",
      "-----------------------------\n",
      "Error: [[0.63949999]]\n",
      "Pred: [array([[0.93216001]]), array([[0.37444607]]), array([[0.41460985]]), array([[0.26772874]])]\n",
      "-----------------------------\n",
      "Error: [[0.63423116]]\n",
      "Pred: [array([[0.84777575]]), array([[0.40518258]]), array([[0.42610404]]), array([[0.27511184]])]\n",
      "-----------------------------\n",
      "Error: [[0.59697648]]\n",
      "Pred: [array([[0.91928756]]), array([[0.4229313]]), array([[0.42467538]]), array([[0.27767701]])]\n",
      "-----------------------------\n",
      "Error: [[0.57143026]]\n",
      "Pred: [array([[0.94621236]]), array([[0.4368505]]), array([[0.41930547]]), array([[0.27492309]])]\n",
      "-----------------------------\n",
      "Error: [[0.54751251]]\n",
      "Pred: [array([[0.95527654]]), array([[0.45112134]]), array([[0.41352285]]), array([[0.27063517]])]\n",
      "-----------------------------\n",
      "Error: [[0.52911879]]\n",
      "Pred: [array([[0.95765092]]), array([[0.46661822]]), array([[0.41566939]]), array([[0.26466616]])]\n",
      "-----------------------------\n",
      "Error: [[0.52028271]]\n",
      "Pred: [array([[0.86264579]]), array([[0.48931053]]), array([[0.40875089]]), array([[0.27117431]])]\n",
      "-----------------------------\n",
      "Error: [[0.48544436]]\n",
      "Pred: [array([[0.92580925]]), array([[0.50363249]]), array([[0.40204404]]), array([[0.268179]])]\n",
      "-----------------------------\n",
      "Error: [[0.48043417]]\n",
      "Pred: [array([[0.8267575]]), array([[0.51331012]]), array([[0.3896314]]), array([[0.24847846]])]\n",
      "-----------------------------\n",
      "Error: [[0.42548956]]\n",
      "Pred: [array([[0.93168371]]), array([[0.53005514]]), array([[0.38041349]]), array([[0.23507414]])]\n",
      "-----------------------------\n",
      "Error: [[0.39057764]]\n",
      "Pred: [array([[0.97585392]]), array([[0.54995543]]), array([[0.37174364]]), array([[0.22194855]])]\n",
      "-----------------------------\n",
      "Error: [[0.35838408]]\n",
      "Pred: [array([[0.99188583]]), array([[0.57182444]]), array([[0.36272656]]), array([[0.20835876]])]\n",
      "-----------------------------\n",
      "Error: [[0.32618124]]\n",
      "Pred: [array([[0.99732464]]), array([[0.59503441]]), array([[0.35288998]]), array([[0.19402478]])]\n",
      "-----------------------------\n",
      "Error: [[0.29395962]]\n",
      "Pred: [array([[0.99912364]]), array([[0.61922271]]), array([[0.34198168]]), array([[0.17893028]])]\n",
      "-----------------------------\n",
      "Error: [[0.26209322]]\n",
      "Pred: [array([[0.99971356]]), array([[0.64414342]]), array([[0.32987654]]), array([[0.1632198]])]\n",
      "-----------------------------\n",
      "Error: [[0.23101666]]\n",
      "Pred: [array([[0.99990644]]), array([[0.66958722]]), array([[0.31653503]]), array([[0.14713807]])]\n",
      "-----------------------------\n",
      "Error: [[0.20117243]]\n",
      "Pred: [array([[0.99996945]]), array([[0.69533761]]), array([[0.30198601]]), array([[0.13098743]])]\n",
      "-----------------------------\n",
      "Error: [[0.17298317]]\n",
      "Pred: [array([[0.99999002]]), array([[0.72114927]]), array([[0.28631986]]), array([[0.11509288]])]\n",
      "-----------------------------\n",
      "Error: [[0.14682342]]\n",
      "Pred: [array([[0.99999674]]), array([[0.7467426]]), array([[0.26968445]]), array([[0.09977179]])]\n",
      "-----------------------------\n",
      "Error: [[0.12299256]]\n",
      "Pred: [array([[0.99999894]]), array([[0.7718111]]), array([[0.2522794]]), array([[0.08530823]])]\n",
      "-----------------------------\n",
      "Error: [[0.10169306]]\n",
      "Pred: [array([[0.99999965]]), array([[0.79603862]]), array([[0.23434668]]), array([[0.07193365]])]\n",
      "-----------------------------\n",
      "Error: [[0.08301831]]\n",
      "Pred: [array([[0.99999989]]), array([[0.81912244]]), array([[0.21615692]]), array([[0.05981475]])]\n",
      "-----------------------------\n",
      "Error: [[0.06695239]]\n",
      "Pred: [array([[0.99999996]]), array([[0.84079717]]), array([[0.19799251]]), array([[0.04904907]])]\n",
      "-----------------------------\n",
      "Error: [[0.05338153]]\n",
      "Pred: [array([[0.99999999]]), array([[0.86085471]]), array([[0.18012943]]), array([[0.03966738]])]\n",
      "-----------------------------\n",
      "Error: [[0.04211477]]\n",
      "Pred: [array([[1.]]), array([[0.87915669]]), array([[0.16282039]]), array([[0.03164155]])]\n",
      "-----------------------------\n",
      "Error: [[0.03290944]]\n",
      "Pred: [array([[1.]]), array([[0.8956381]]), array([[0.14628131]]), array([[0.0248961]])]\n",
      "-----------------------------\n",
      "Error: [[0.02549692]]\n",
      "Pred: [array([[1.]]), array([[0.91030257]]), array([[0.13068269]]), array([[0.01932155]])]\n",
      "-----------------------------\n",
      "Error: [[0.01960505]]\n",
      "Pred: [array([[1.]]), array([[0.92321167]]), array([[0.11614613]]), array([[0.01478777]])]\n",
      "-----------------------------\n",
      "Error: [[0.01497515]]\n",
      "Pred: [array([[1.]]), array([[0.9344708]]), array([[0.10274543]]), array([[0.01115592]])]\n",
      "-----------------------------\n",
      "Error: [[0.01137301]]\n",
      "Pred: [array([[1.]]), array([[0.9442145]]), array([[0.09051129]]), array([[0.00828819]])]\n",
      "-----------------------------\n",
      "Error: [[0.00859449]]\n",
      "Pred: [array([[1.]]), array([[0.95259288]]), array([[0.079438]]), array([[0.0060548]])]\n",
      "-----------------------------\n",
      "Error: [[0.00646705]]\n",
      "Pred: [array([[1.]]), array([[0.95976056]]), array([[0.06949115]]), array([[0.00433851]])]\n",
      "-----------------------------\n",
      "Error: [[0.00484839]]\n",
      "Pred: [array([[1.]]), array([[0.96586842]]), array([[0.06061519]]), array([[0.0030369]])]\n",
      "-----------------------------\n",
      "Error: [[0.00362344]]\n",
      "Pred: [array([[1.]]), array([[0.97105808]]), array([[0.05274037]]), array([[0.00206292]])]\n",
      "-----------------------------\n",
      "Error: [[0.00270068]]\n",
      "Pred: [array([[1.]]), array([[0.97545861]]), array([[0.04578857]]), array([[0.00134428]])]\n",
      "-----------------------------\n",
      "Error: [[0.00200828]]\n",
      "Pred: [array([[1.]]), array([[0.97918514]]), array([[0.03967801]]), array([[0.00082215]])]\n",
      "-----------------------------\n",
      "Error: [[0.00149045]]\n",
      "Pred: [array([[1.]]), array([[0.98233865]]), array([[0.03432677]]), array([[0.00044941]])]\n",
      "-----------------------------\n",
      "Error: [[0.00110428]]\n",
      "Pred: [array([[1.]]), array([[0.98500656]]), array([[0.02965541]]), array([[0.00018891]])]\n",
      "-----------------------------\n",
      "Error: [[0.00081699]]\n",
      "Pred: [array([[1.]]), array([[0.9872639]]), array([[0.0255887]]), array([[1.17417354e-05]])]\n",
      "-----------------------------\n",
      "Error: [[0.00060369]]\n",
      "Pred: [array([[1.]]), array([[0.98917452]]), array([[0.02205665]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Error: [[0.00044603]]\n",
      "Pred: [array([[1.]]), array([[0.99074203]]), array([[0.01898222]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Error: [[0.00032927]]\n",
      "Pred: [array([[1.]]), array([[0.99206334]]), array([[0.01631798]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Error: [[0.00024279]]\n",
      "Pred: [array([[1.]]), array([[0.99319242]]), array([[0.0140159]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Error: [[0.00017882]]\n",
      "Pred: [array([[1.]]), array([[0.99416156]]), array([[0.01203045]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Error: [[0.00013157]]\n",
      "Pred: [array([[1.]]), array([[0.99499431]]), array([[0.01032039]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Error: [[9.67131108e-05]]\n",
      "Pred: [array([[1.]]), array([[0.99570979]]), array([[0.00884913]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Error: [[7.10356811e-05]]\n",
      "Pred: [array([[1.]]), array([[0.99632422]]), array([[0.00758448]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Error: [[5.21397337e-05]]\n",
      "Pred: [array([[1.]]), array([[0.99685158]]), array([[0.00649825]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Error: [[3.82475378e-05]]\n",
      "Pred: [array([[1.]]), array([[0.99730399]]), array([[0.00556588]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Error: [[2.80424916e-05]]\n",
      "Pred: [array([[1.]]), array([[0.9976919]]), array([[0.00476604]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Error: [[2.05513031e-05]]\n",
      "Pred: [array([[1.]]), array([[0.99802439]]), array([[0.00408023]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Error: [[1.50556227e-05]]\n",
      "Pred: [array([[1.]]), array([[0.99830926]]), array([[0.00349242]]), array([[0.]])]\n",
      "-----------------------------\n",
      "Weight: {'weights_0_1': array([[-0.16595599,  0.91056768, -0.99977125, -0.90023925],\n",
      "       [-0.70648822, -0.92794391, -0.62747958,  0.89703458],\n",
      "       [-0.20646505, -0.03308324, -0.16161097,  0.00237016]]), 'weights_1_2': array([[-0.5910955 ],\n",
      "       [ 1.13962134],\n",
      "       [-0.94522481],\n",
      "       [ 1.11023793]])}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "alpha = .2\n",
    "hidden_size = 4\n",
    "iterations = 60\n",
    "\n",
    "goal_pred = np.array([[ 1, 1, 0, 0 ]]).T;\n",
    "\n",
    "weights_0_1 = 2 * np.random.random((3, hidden_size)) - 1\n",
    "weights_1_2 = 2 * np.random.random((hidden_size, 1)) - 1\n",
    "\n",
    "weights = {\n",
    "            'weights_0_1': weights_0_1,\n",
    "            'weights_1_2': weights_1_2\n",
    "          }\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output > 0\n",
    "\n",
    "def neural_net(input: np.ndarray, weights: dict):\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        error = 0\n",
    "        preds = []\n",
    "        \n",
    "        for index, obs in enumerate(input):\n",
    "            layer_0 = input[index:index+1]\n",
    "            layer_1 = relu(layer_0.dot(weights['weights_0_1']))\n",
    "            layer_2 = layer_1.dot(weights['weights_1_2'])\n",
    "            \n",
    "            preds.append(layer_2)\n",
    "            layer_2_delta = layer_2 - goal_pred[index:index+1]\n",
    "            layer_1_delta = layer_2_delta.dot(weights['weights_1_2'].T) * relu2deriv(layer_1)\n",
    "            error += (layer_2_delta ** 2)\n",
    "            \n",
    "            weights_1_2_delta = layer_1.T.dot(layer_2_delta)\n",
    "            weights_0_1_delta = layer_0.T.dot(layer_1_delta)\n",
    "            \n",
    "            weights['weights_1_2'] -= (weights_1_2_delta * alpha)\n",
    "            weights['weights_0_1'] -= (weights_0_1_delta * alpha)            \n",
    "            \n",
    "        print('Error: ' + str(error))\n",
    "        print('Pred: ' + str(preds) + '\\n-----------------------------')\n",
    "    \n",
    "    print('Weight: ' + str(weights))       \n",
    "    \n",
    "neural_net(streetlights, weights)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1);\n",
    "hidden_size = 5\n",
    "alpha = .0005\n",
    "\n",
    "def relu(x):\n",
    "   return (x > 0) * x\n",
    "\n",
    "def relu2deriv(x):\n",
    "   return x > 0\n",
    "\n",
    "goal_pred = np.reshape(goal_pred, (6, 1))\n",
    "synapse_0_1 = np.random.randint(0, 12, (3, hidden_size))\n",
    "synapse_1_2 = np.random.rand(hidden_size, 1)\n",
    "\n",
    "for i in range(3000):\n",
    "    layer_1 = relu(streetlights.dot(synapse_0_1))\n",
    "    layer_2 = layer_1.dot(synapse_1_2)\n",
    "    layer_2_delta = layer_2 - goal_pred\n",
    "    msquared_error = layer_2_delta ** 2\n",
    "    layer_1_delta = layer_2_delta.dot(synapse_1_2.T) * relu2deriv(layer_1)\n",
    "    synapse_1_2_delta = layer_1.T.dot(layer_2_delta)  \n",
    "    synapse_0_1_delta = streetlights.T.dot(layer_1_delta)\n",
    "    synapse_1_2 = synapse_1_2 - synapse_1_2_delta * alpha\n",
    "    synapse_0_1 = synapse_0_1 - synapse_0_1_delta * alpha\n",
    "    if (i % 10 == 0):\n",
    "        print('\\nPred: ' + str(layer_2.T) + ' Error: ' + str(msquared_error.T) + ' Goal: ' + str(goal_pred.T))\n",
    "    #print('\\nLayer_1: ' + str(layer_1) + ' Synapse_1_2: ' + str(synapse_1_2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can learn correlation:\n",
    "> In the previous example the network identified correlation between the middle input and the output;\n",
    "The Correlation is located whatever the weights were set to higher numbers - Signal;\n",
    "Inversely, _randomness_ with respect to the output was found at the far left and far right weights(where the weight values are near to 0). - Noise;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross comunication between neurons occurs by sharing the same error measurement.\n",
    ">It makes it clear that our weight update is nothing more than taking this shared error measument\n",
    "and multiplying it by each respective input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full-batch Gradient Descent: \n",
    ">Updating weights one dataset at a time;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Gradient Descent:\n",
    ">Updating weights after 'n' observations;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation: Long distance error attribuition**\n",
    "\n",
    "The \"weighted average error\"\n",
    "\n",
    "Bottom line: Trask use the fact that we want to know how to calculate delta to our lower layer(layer_0 to layer_1), in a way that these values can  help our upper layer(layer_2) to predict accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"moving delta signal around\" - weighted average delta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
